{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Network Based Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Extraction Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import librosa\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the UrbanSound8K dataset\n",
    "dataset_path = '/home/nathanael-seay/Downloads/Urban Sound/UrbanSound8K'\n",
    "metadata_path = os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv')\n",
    "audio_folder_path = os.path.join(dataset_path, 'audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata file\n",
    "metadata = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load YAMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAMNet model from TensorFlow Hub\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yamnet_features(file_name, spectrogram_save_path):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        audio_data, sr = librosa.load(file_name, sr=16000)  # YAMNet expects 16kHz audio\n",
    "        # Ensure audio is 1D\n",
    "        if len(audio_data.shape) > 1:\n",
    "            audio_data = np.mean(audio_data, axis=1)\n",
    "        # Extract YAMNet embeddings\n",
    "        scores, embeddings, spectrogram = yamnet_model(audio_data)\n",
    "        # Average the embeddings\n",
    "        embeddings = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "\n",
    "        # Normalize and convert the spectrogram data\n",
    "        spectrogram_data = spectrogram.numpy()\n",
    "        spectrogram_data -= spectrogram_data.min()\n",
    "        spectrogram_data /= spectrogram_data.max()\n",
    "        spectrogram_data *= 255.0\n",
    "        spectrogram_data = spectrogram_data.astype(np.uint8)\n",
    "\n",
    "         # Save the spectrogram without plotting using Pillow\n",
    "        image = Image.fromarray(spectrogram_data)\n",
    "        image.save(spectrogram_save_path)\n",
    "        return embeddings\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_name}, {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the spectrograms directory exists\n",
    "os.makedirs('spectrograms', exist_ok=True)\n",
    "\n",
    "# Extract features and store in a list\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(audio_folder_path, f'fold{row[\"fold\"]}', row[\"slice_file_name\"])\n",
    "    class_label = row[\"class\"]\n",
    "    spectrogram_save_path = os.path.join('spectrograms', f'{row[\"slice_file_name\"]}.png')\n",
    "    data = extract_yamnet_features(file_name, spectrogram_save_path)\n",
    "    if data is not None:\n",
    "        features.append(data)\n",
    "        labels.append(class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a DataFrame\n",
    "features_df = pd.DataFrame(features)\n",
    "labels_df = pd.Series(labels, name='label')\n",
    "\n",
    "# Combine features and labels\n",
    "final_df = pd.concat([features_df, labels_df], axis=1)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "final_df.to_csv('urbansound8k_yamnet_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency Matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "Edge List:\n",
      "[[   0 6563]\n",
      " [   0 8349]\n",
      " [   0 8129]\n",
      " ...\n",
      " [8731 1953]\n",
      " [8731 1954]\n",
      " [8731 5956]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the feature dataset\n",
    "data = pd.read_csv('urbansound8k_yamnet_features.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# Number of neighbors\n",
    "k_neighbors = 5\n",
    "\n",
    "# Construct the KNN graph\n",
    "knn_graph = kneighbors_graph(X, n_neighbors=k_neighbors, mode='connectivity', include_self=False)\n",
    "adj_matrix = knn_graph.toarray()\n",
    "\n",
    "# Create adjacency matrix and edge list\n",
    "edges = np.array(knn_graph.nonzero()).T\n",
    "node_features = X.values\n",
    "\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adj_matrix)\n",
    "print(\"\\nEdge List:\")\n",
    "print(edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into train-test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Edges:\n",
      "[[ 677 7982]\n",
      " [6565 1136]\n",
      " [3772 3791]\n",
      " ...\n",
      " [ 769  775]\n",
      " [2925 8389]\n",
      " [4854 4564]]\n",
      "\n",
      "Test Edges:\n",
      "[[5325 5328]\n",
      " [8318 3192]\n",
      " [1007 3660]\n",
      " ...\n",
      " [3104 3107]\n",
      " [1625 1591]\n",
      " [3966  921]]\n",
      "\n",
      "Train Negative Edges:\n",
      "[[6572 1707]\n",
      " [8124 1319]\n",
      " [1873 2193]\n",
      " ...\n",
      " [6490 7057]\n",
      " [4623 7439]\n",
      " [5898 3678]]\n",
      "\n",
      "Test Negative Edges:\n",
      "[[3017 4575]\n",
      " [2760 7395]\n",
      " [8687  424]\n",
      " ...\n",
      " [3137 2329]\n",
      " [8538 7265]\n",
      " [1080  281]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to generate negative samples\n",
    "def generate_negative_samples(adj_matrix, num_samples):\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < num_samples:\n",
    "        u, v = random.sample(range(adj_matrix.shape[0]), 2)\n",
    "        if adj_matrix[u, v] == 0:\n",
    "            neg_samples.append((u, v))\n",
    "    return np.array(neg_samples)\n",
    "\n",
    "# Split edges into train/test sets\n",
    "num_edges = len(edges)\n",
    "num_test = int(0.2 * num_edges)\n",
    "num_train = num_edges - num_test\n",
    "\n",
    "np.random.shuffle(edges)\n",
    "train_edges = edges[:num_train]\n",
    "test_edges = edges[num_train:]\n",
    "\n",
    "# Generate negative samples\n",
    "train_neg_edges = generate_negative_samples(adj_matrix, num_train)\n",
    "test_neg_edges = generate_negative_samples(adj_matrix, num_test)\n",
    "\n",
    "print(\"\\nTrain Edges:\")\n",
    "print(train_edges)\n",
    "print(\"\\nTest Edges:\")\n",
    "print(test_edges)\n",
    "print(\"\\nTrain Negative Edges:\")\n",
    "print(train_neg_edges)\n",
    "print(\"\\nTest Negative Edges:\")\n",
    "print(test_neg_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "    def call(self, adj_matrix, node_features):\n",
    "        # Perform the graph convolution operation\n",
    "        support = tf.matmul(adj_matrix, node_features)\n",
    "        output = self.dense(support)\n",
    "        return tf.nn.relu(output)\n",
    "\n",
    "class GCN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNLayer(hidden_dim, output_dim)\n",
    "\n",
    "    def call(self, adj_matrix, node_features):\n",
    "        h = self.gcn1(adj_matrix, node_features)\n",
    "        h = self.gcn2(adj_matrix, h)\n",
    "        return h\n",
    "\n",
    "# Create the model\n",
    "input_dim = node_features.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 64\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.6931463479995728\n",
      "Epoch 20/100, Loss: 0.6931463479995728\n",
      "Epoch 30/100, Loss: 0.6931463479995728\n",
      "Epoch 40/100, Loss: 0.6931463479995728\n",
      "Epoch 50/100, Loss: 0.6931463479995728\n",
      "Epoch 60/100, Loss: 0.6931463479995728\n",
      "Epoch 70/100, Loss: 0.6931463479995728\n",
      "Epoch 80/100, Loss: 0.6931463479995728\n",
      "Epoch 90/100, Loss: 0.6931463479995728\n",
      "Epoch 100/100, Loss: 0.6931463479995728\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    labels = tf.concat([tf.ones_like(pos_score), tf.zeros_like(neg_score)], 0)\n",
    "    logits = tf.concat([pos_score, neg_score], 0)\n",
    "    return loss_fn(labels, logits)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, adj_matrix, node_features, train_edges, train_neg_edges):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Positive edge predictions\n",
    "        h = model(adj_matrix, node_features)\n",
    "        pos_u, pos_v = train_edges[:, 0], train_edges[:, 1]\n",
    "        pos_score = tf.reduce_sum(tf.gather(h, pos_u) * tf.gather(h, pos_v), axis=1)\n",
    "        \n",
    "        # Negative edge predictions\n",
    "        neg_u, neg_v = train_neg_edges[:, 0], train_neg_edges[:, 1]\n",
    "        neg_score = tf.reduce_sum(tf.gather(h, neg_u) * tf.gather(h, neg_v), axis=1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(pos_score, neg_score)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_step(model, adj_matrix, node_features, train_edges, train_neg_edges)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_27194/749866727.py\", line 17, in test_step  *\n        return compute_auc(pos_score, neg_score)\n    File \"/tmp/ipykernel_27194/749866727.py\", line 5, in compute_auc  *\n        return tf.keras.metrics.AUC()(labels, scores)\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 1288, in __init__  **\n        self._build(None)\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 1312, in _build\n        self.true_positives = self.add_variable(\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/metrics/metric.py\", line 192, in add_variable\n        variable = backend.Variable(\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/backend/common/variables.py\", line 165, in __init__\n        self._initialize(value)\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py\", line 31, in _initialize\n        self._value = tf.Variable(\n\n    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     neg_score \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39mgather(h, neg_u) \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(h, neg_v), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_auc(pos_score, neg_score)\n\u001b[0;32m---> 19\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_neg_edges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/AIenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_z_gaa_o.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_step\u001b[0;34m(model, adj_matrix, node_features, test_edges, test_neg_edges)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(compute_auc), (ag__\u001b[38;5;241m.\u001b[39mld(pos_score), ag__\u001b[38;5;241m.\u001b[39mld(neg_score)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filelv_kpyo5.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_auc\u001b[0;34m(pos_score, neg_score)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), (ag__\u001b[38;5;241m.\u001b[39mld(labels), ag__\u001b[38;5;241m.\u001b[39mld(scores)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/AIenv/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py:1288\u001b[0m, in \u001b[0;36mAUC.__init__\u001b[0;34m(self, num_thresholds, curve, summation_method, name, dtype, thresholds, multi_label, num_labels, label_weights, from_logits)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_labels:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_labels` is needed only when `multi_label` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1287\u001b[0m     )\n\u001b[0;32m-> 1288\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIenv/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py:1312\u001b[0m, in \u001b[0;36mAUC._build\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_input_shape \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Create metric variables\u001b[39;00m\n\u001b[0;32m-> 1312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_positives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue_positives\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfalse_positives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_variable(\n\u001b[1;32m   1318\u001b[0m     shape\u001b[38;5;241m=\u001b[39mvariable_shape,\n\u001b[1;32m   1319\u001b[0m     initializer\u001b[38;5;241m=\u001b[39minitializers\u001b[38;5;241m.\u001b[39mZeros(),\n\u001b[1;32m   1320\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse_positives\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1321\u001b[0m )\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_negatives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_variable(\n\u001b[1;32m   1323\u001b[0m     shape\u001b[38;5;241m=\u001b[39mvariable_shape,\n\u001b[1;32m   1324\u001b[0m     initializer\u001b[38;5;241m=\u001b[39minitializers\u001b[38;5;241m.\u001b[39mZeros(),\n\u001b[1;32m   1325\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_negatives\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1326\u001b[0m )\n",
      "File \u001b[0;32m~/AIenv/lib/python3.11/site-packages/keras/src/metrics/metric.py:192\u001b[0m, in \u001b[0;36mMetric.add_variable\u001b[0;34m(self, shape, initializer, dtype, aggregation, name)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m), caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    191\u001b[0m     initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(initializer)\n\u001b[0;32m--> 192\u001b[0m     variable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Prevent double-tracking\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracker\u001b[38;5;241m.\u001b[39madd_to_store(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m, variable)\n",
      "File \u001b[0;32m~/AIenv/lib/python3.11/site-packages/keras/src/backend/common/variables.py:165\u001b[0m, in \u001b[0;36mKerasVariable.__init__\u001b[0;34m(self, initializer, shape, dtype, trainable, autocast, aggregation, name)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         value \u001b[38;5;241m=\u001b[39m initializer\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape)\n",
      "File \u001b[0;32m~/AIenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:31\u001b[0m, in \u001b[0;36mVariable._initialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_27194/749866727.py\", line 17, in test_step  *\n        return compute_auc(pos_score, neg_score)\n    File \"/tmp/ipykernel_27194/749866727.py\", line 5, in compute_auc  *\n        return tf.keras.metrics.AUC()(labels, scores)\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 1288, in __init__  **\n        self._build(None)\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 1312, in _build\n        self.true_positives = self.add_variable(\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/metrics/metric.py\", line 192, in add_variable\n        variable = backend.Variable(\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/backend/common/variables.py\", line 165, in __init__\n        self._initialize(value)\n    File \"/home/nathanael-seay/AIenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py\", line 31, in _initialize\n        self._value = tf.Variable(\n\n    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = tf.concat([pos_score, neg_score], 0)\n",
    "    labels = tf.concat([tf.ones_like(pos_score), tf.zeros_like(neg_score)], 0)\n",
    "    return tf.keras.metrics.AUC()(labels, scores)\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, adj_matrix, node_features, test_edges, test_neg_edges):\n",
    "    h = model(adj_matrix, node_features)\n",
    "    \n",
    "    pos_u, pos_v = test_edges[:, 0], test_edges[:, 1]\n",
    "    pos_score = tf.reduce_sum(tf.gather(h, pos_u) * tf.gather(h, pos_v), axis=1)\n",
    "    \n",
    "    neg_u, neg_v = test_neg_edges[:, 0], test_neg_edges[:, 1]\n",
    "    neg_score = tf.reduce_sum(tf.gather(h, neg_u) * tf.gather(h, neg_v), axis=1)\n",
    "    \n",
    "    return compute_auc(pos_score, neg_score)\n",
    "\n",
    "auc = test_step(model, adj_matrix, node_features, test_edges, test_neg_edges)\n",
    "print(f'Test AUC: {auc.numpy()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
